{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11126425,"sourceType":"datasetVersion","datasetId":6938910},{"sourceId":11150864,"sourceType":"datasetVersion","datasetId":6956921},{"sourceId":11160659,"sourceType":"datasetVersion","datasetId":6963996},{"sourceId":11160884,"sourceType":"datasetVersion","datasetId":6779454},{"sourceId":11177421,"sourceType":"datasetVersion","datasetId":6975364},{"sourceId":90869,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":76178,"modelId":100857},{"sourceId":301365,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":257385,"modelId":278686},{"sourceId":301325,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":257352,"modelId":278653}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ckapelonis02/sam2-fine-tune.git\n%cd /kaggle/working/sam2-fine-tune","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:39:55.156023Z","iopub.execute_input":"2025-03-27T11:39:55.156339Z","iopub.status.idle":"2025-03-27T11:39:58.261289Z","shell.execute_reply.started":"2025-03-27T11:39:55.156315Z","shell.execute_reply":"2025-03-27T11:39:58.260447Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'sam2-fine-tune'...\nremote: Enumerating objects: 355, done.\u001b[K\nremote: Counting objects: 100% (71/71), done.\u001b[K\nremote: Compressing objects: 100% (52/52), done.\u001b[K\nremote: Total 355 (delta 39), reused 49 (delta 19), pack-reused 284 (from 1)\u001b[K\nReceiving objects: 100% (355/355), 82.99 MiB | 44.61 MiB/s, done.\nResolving deltas: 100% (45/45), done.\n/kaggle/working/sam2-fine-tune\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%pip install -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:39:58.262667Z","iopub.execute_input":"2025-03-27T11:39:58.262988Z","iopub.status.idle":"2025-03-27T11:43:07.359397Z","shell.execute_reply.started":"2025-03-27T11:39:58.262956Z","shell.execute_reply":"2025-03-27T11:43:07.358440Z"}},"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/sam2-fine-tune\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (0.20.1+cu121)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (1.26.4)\nRequirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (4.67.1)\nCollecting hydra-core>=1.3.2 (from SAM-2==1.0)\n  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting iopath>=0.1.10 (from SAM-2==1.0)\n  Using cached iopath-0.1.10.tar.gz (42 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (11.0.0)\nRequirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\nCollecting portalocker (from iopath>=0.1.10->SAM-2==1.0)\n  Using cached portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->SAM-2==1.0) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.1->SAM-2==1.0) (1.3.0)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.1->SAM-2==1.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->SAM-2==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->SAM-2==1.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->SAM-2==1.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->SAM-2==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->SAM-2==1.0) (2024.2.0)\nUsing cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\nUsing cached portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: SAM-2, iopath\n  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for SAM-2: filename=sam_2-1.0-0.editable-cp310-cp310-linux_x86_64.whl size=11430 sha256=e8b14d7a123e028e5c5f246d4e44d2df169554a488fce8d35704bd4397641245\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ngqej081/wheels/20/fd/7b/feee66c2bf44c9692bb2e1013d443bca1548170c9235392525\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=2b290181243d05103bbe6a861f55fbbb26c4181e29d594aa244cc85c513f2277\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built SAM-2 iopath\nInstalling collected packages: portalocker, iopath, hydra-core, SAM-2\nSuccessfully installed SAM-2-1.0 hydra-core-1.3.2 iopath-0.1.10 portalocker-3.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import kagglehub\npath = kagglehub.model_download(\"metaresearch/segment-anything-2/pyTorch/sam2-hiera-tiny\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:43:07.361042Z","iopub.execute_input":"2025-03-27T11:43:07.361331Z","iopub.status.idle":"2025-03-27T11:43:08.163784Z","shell.execute_reply.started":"2025-03-27T11:43:07.361307Z","shell.execute_reply":"2025-03-27T11:43:08.162881Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-tiny/1/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:43:08.164727Z","iopub.execute_input":"2025-03-27T11:43:08.164989Z","iopub.status.idle":"2025-03-27T11:43:08.168666Z","shell.execute_reply.started":"2025-03-27T11:43:08.164965Z","shell.execute_reply":"2025-03-27T11:43:08.167607Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import hydra\nimport numpy as np\nimport torch\nimport cv2\nimport os\nimport random\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\nfrom sam2.train_helper import *\n\ncleanup()\n\n# Configurations\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nhydra.core.global_hydra.GlobalHydra.instance().clear()\nhydra.initialize_config_module('sam2', version_base='1.2')\n\n# Model Initialization\nsam2_model = build_sam2(\n    config_file=\"../sam2_configs/sam2_hiera_t.yaml\",\n    ckpt_path=\"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-tiny/1/sam2_hiera_tiny.pt\",\n    device=\"cuda\",\n    apply_postprocessing=False\n)\npredictor = SAM2ImagePredictor(sam2_model)\npredictor.model.sam_mask_decoder.train(True)\npredictor.model.sam_prompt_encoder.train(True)\n\n# Optimizer & Scheduler\noptimizer = optim.AdamW(predictor.model.parameters(), lr=1e-5, weight_decay=4e-5)\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10000, eta_min=1e-7)\nscaler = torch.cuda.amp.GradScaler()\n\n# Dataset Configuration\ndata_size = 2000\nfile_names = list(range(1, data_size + 1))\nrandom.shuffle(file_names)\ntrain_size = int(0.8 * data_size)\ntrain_files, val_files = file_names[:train_size], file_names[train_size:]\n\ntrain_data = read_dataset(\"/kaggle/input/data-2k-cropped/images\", \"/kaggle/input/data-2k-cropped/masks\", train_files)\nval_data = read_dataset(\"/kaggle/input/data-2k-cropped/images\", \"/kaggle/input/data-2k-cropped/masks\", val_files)\n\n# Training Parameters\nmax_masks = 150\nepochs = 10\nbest_val_iou = 0.0\ngradient_accumulation_steps = 4\n\ndef process_batch(image, masks, input_point, input_label):\n    \"\"\"Processes a single batch and returns the predicted masks, scores, and ground truth masks.\"\"\"\n    if masks.shape[0] == 0:\n        return None, None, None\n\n    predictor.set_image(image)\n    \n    mask_input, unnorm_coords, labels, _ = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n    sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels), boxes=None, masks=None)\n\n    batched_mode = unnorm_coords.shape[0] > 1\n    high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n\n    low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n        image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n        image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n        sparse_prompt_embeddings=sparse_embeddings,\n        dense_prompt_embeddings=dense_embeddings,\n        multimask_output=True,\n        repeat_image=batched_mode,\n        high_res_features=high_res_features\n    )\n\n    prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n    gt_mask = torch.tensor((masks / 255).astype(np.float16), device=\"cuda\")\n    prd_mask = torch.sigmoid(prd_masks[:, 0].to(dtype=torch.float16))\n\n    return prd_mask, prd_scores, gt_mask\n\ndef compute_iou_loss(prd_mask, prd_scores, gt_mask):\n    \"\"\"Computes IoU, segmentation loss, and score loss.\"\"\"\n    inter = (gt_mask * (prd_mask > 0.5)).sum(dim=[1, 2])\n    union = gt_mask.sum(dim=[1, 2]) + (prd_mask > 0.5).sum(dim=[1, 2]) - inter\n    iou = inter / (union + 1e-6)\n    \n    seg_loss = (-gt_mask * torch.log(prd_mask + 1e-6) - (1 - gt_mask) * torch.log((1 - prd_mask) + 1e-6)).mean()\n    score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n    \n    return iou, seg_loss + score_loss * 0.05\n\ndef evaluate():\n    \"\"\"Evaluates the model on the validation dataset.\"\"\"\n    predictor.model.eval()\n    total_iou, count = 0, 0\n\n    with torch.no_grad():\n        for i in tqdm(range(len(val_files)), desc=\"Validation Progress\"):\n            image, masks, input_point, input_label = read_batch(val_data, i, max_masks)\n            prd_mask, prd_scores, gt_mask = process_batch(image, masks, input_point, input_label)\n            \n            if prd_mask is None:\n                continue\n            \n            iou, _ = compute_iou_loss(prd_mask, prd_scores, gt_mask)\n            total_iou += iou.mean().item()\n            count += 1\n\n    predictor.model.train()\n    return total_iou / count if count > 0 else 0\n\n# Training Loop\nfor epoch in range(epochs):\n    mean_iou = 0\n    random.shuffle(train_files)\n    \n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n    for itr in tqdm(range(train_size), desc=\"Training Progress\"):\n        with torch.cuda.amp.autocast():\n            image, masks, input_point, input_label = read_batch(train_data, itr % train_size, max_masks)\n            prd_mask, prd_scores, gt_mask = process_batch(image, masks, input_point, input_label)\n\n            if prd_mask is None:\n                continue\n\n            iou, loss = compute_iou_loss(prd_mask, prd_scores, gt_mask)\n            loss = loss / gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n\n            if (itr + 1) % gradient_accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                predictor.model.zero_grad()\n\n            scheduler.step()\n            mean_iou = mean_iou * 0.99 + 0.01 * iou.mean().item()\n\n    val_iou = evaluate()\n    print(f\"Epoch {epoch+1}: Train IoU = {mean_iou:.4f}, Val IoU = {val_iou:.4f}\")\n\n    if val_iou > best_val_iou:\n        best_val_iou = val_iou\n        torch.save(predictor.model.state_dict(), \"best_model.torch\")\n        print(f\"New best model saved, Val IoU = {best_val_iou:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:55:53.591248Z","iopub.execute_input":"2025-03-27T11:55:53.591563Z","iopub.status.idle":"2025-03-27T14:36:36.131389Z","shell.execute_reply.started":"2025-03-27T11:55:53.591539Z","shell.execute_reply":"2025-03-27T14:36:36.129967Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 1600/1600 [22:37<00:00,  1.18it/s]\nValidation Progress: 100%|██████████| 400/400 [03:45<00:00,  1.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train IoU = 0.8744, Val IoU = 0.8616\nNew best model saved, Val IoU = 0.8616\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 1600/1600 [22:07<00:00,  1.21it/s]\nValidation Progress: 100%|██████████| 400/400 [03:37<00:00,  1.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train IoU = 0.8946, Val IoU = 0.8797\nNew best model saved, Val IoU = 0.8797\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 1600/1600 [22:08<00:00,  1.20it/s]\nValidation Progress: 100%|██████████| 400/400 [03:38<00:00,  1.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train IoU = 0.9027, Val IoU = 0.8874\nNew best model saved, Val IoU = 0.8874\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 1600/1600 [22:11<00:00,  1.20it/s]\nValidation Progress: 100%|██████████| 400/400 [03:38<00:00,  1.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train IoU = 0.9063, Val IoU = 0.8909\nNew best model saved, Val IoU = 0.8909\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 1600/1600 [22:10<00:00,  1.20it/s]\nValidation Progress: 100%|██████████| 400/400 [03:38<00:00,  1.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train IoU = 0.9078, Val IoU = 0.8924\nNew best model saved, Val IoU = 0.8924\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 1600/1600 [22:09<00:00,  1.20it/s]\nValidation Progress: 100%|██████████| 400/400 [03:37<00:00,  1.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train IoU = 0.9082, Val IoU = 0.8929\nNew best model saved, Val IoU = 0.8929\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training Progress:  24%|██▍       | 380/1600 [05:17<16:59,  1.20it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-1ae33fcc10f2>\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mprd_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprd_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprd_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-1ae33fcc10f2>\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(image, masks, input_point, input_label)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mprd_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_res_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_hw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mprd_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprd_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"!mkdir /kaggle/working/sam2-fine-tune/results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:36:42.179298Z","iopub.execute_input":"2025-03-27T14:36:42.179597Z","iopub.status.idle":"2025-03-27T14:36:42.348878Z","shell.execute_reply.started":"2025-03-27T14:36:42.179576Z","shell.execute_reply":"2025-03-27T14:36:42.347570Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport cv2\nimport hydra\nimport matplotlib.pyplot as plt\nimport os\nimport time\nfrom PIL import Image\nfrom sam2.build_sam import build_sam2\nfrom sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\nfrom sam2.test_helper import test_generator\nfrom sam2.train_helper import cleanup\n\ncleanup()\n\n# Configurations\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nhydra.core.global_hydra.GlobalHydra.instance().clear()\nhydra.initialize_config_module('sam2', version_base='1.2')\n\nsam2_model = build_sam2(\n    config_file=\"../sam2_configs/sam2_hiera_t.yaml\",\n    ckpt_path=\"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-tiny/1/sam2_hiera_tiny.pt\",\n    device=\"cuda\",\n    apply_postprocessing=False\n)\n\nmask_generator = SAM2AutomaticMaskGenerator(\n    model=sam2_model,\n    points_per_side=32,\n    points_per_batch=4,\n    pred_iou_thresh=0.75,\n    stability_score_thresh=0.92,\n    stability_score_offset=0.91,\n    mask_threshold=0.4,\n    box_nms_thresh=0.7,\n    crop_n_layers=2,\n    crop_nms_thresh=0.7,\n    crop_overlap_ratio=0.3,\n    crop_n_points_downscale_factor=2,\n    point_grids=None,\n    min_mask_region_area=25.0,\n    output_mode=\"binary_mask\",\n    use_m2m=False,\n    multimask_output=True,\n    load_model=\"/kaggle/working/sam2-fine-tune/best_model.torch\"\n)\n\nimport pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv(\"/kaggle/input/evaluation-dataset/crops.csv\")  # Replace with your actual file path\n\n# Access specific columns\nfile_names = df[\"file_name\"]\nrows = df[\"rows\"]\ncols = df[\"cols\"]\n\n# Example: Iterate over the data\nfor file_name, row, col in zip(file_names, rows, cols):\n    print(f\"File: {file_name}, Rows: {row}, Cols: {col}\")\n    start_time = time.time()\n    test_generator(\n        mask_generator=mask_generator,\n        img_path=f\"/kaggle/input/evaluation-dataset/evaluation_dataset/images_set/{file_name}.jpg\",\n        output_path=f\"/kaggle/working/sam2-fine-tune/results/{file_name}_{time.time()}.png\",\n        rows=row,\n        cols=col,\n        max_mask_crop_region=0.1,\n        show_masks=False\n    )\n    print(f\"Time taken: {time.time() - start_time}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:38:13.301232Z","iopub.execute_input":"2025-03-27T14:38:13.301523Z"}},"outputs":[{"name":"stdout","text":"File: butterfly, Rows: 4, Cols: 4\nProcessing 1 of 16\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import optuna\nimport numpy as np\nimport time\nfrom sam2.build_sam import build_sam2\nfrom sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\nfrom sam2.test_helper import test_generator\nfrom evaluate import *\n\ndef objective(trial):\n    points_per_side = 128\n    points_per_batch = 32\n    pred_iou_thresh = trial.suggest_float('pred_iou_thresh', 0.5, 0.9)\n    stability_score_thresh = trial.suggest_float('stability_score_thresh', 0.7, 0.95)\n    stability_score_offset = trial.suggest_float('stability_score_offset', 0.7, 1.2)\n    mask_threshold = trial.suggest_float('mask_threshold', 0.0, 0.6)\n    box_nms_thresh = 0.7\n    crop_n_layers = 2\n    crop_nms_thresh = 0.7\n    crop_overlap_ratio = 0.3\n    crop_n_points_downscale_factor = 2\n    min_mask_region_area = 25.0\n    use_m2m = False\n\n    sam2_model = build_sam2(\n        config_file=\"../sam2_configs/sam2_hiera_t.yaml\",\n        ckpt_path=\"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-tiny/1/sam2_hiera_tiny.pt\",\n        device=\"cuda\",\n        apply_postprocessing=False\n    )\n\n    mask_generator = SAM2AutomaticMaskGenerator(\n        model=sam2_model,\n        points_per_side=points_per_side,\n        points_per_batch=points_per_batch,\n        pred_iou_thresh=pred_iou_thresh,\n        stability_score_thresh=stability_score_thresh,\n        stability_score_offset=stability_score_offset,\n        mask_threshold=mask_threshold,\n        box_nms_thresh=box_nms_thresh,\n        crop_n_layers=crop_n_layers,\n        crop_nms_thresh=crop_nms_thresh,\n        crop_overlap_ratio=crop_overlap_ratio,\n        crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n        min_mask_region_area=min_mask_region_area,\n        use_m2m=use_m2m\n    )\n\n    img_path = \"/kaggle/input/evaluation-dataset/images_set/butterfly.jpg\"\n    output_path = \"/kaggle/working/sam2-fine-tune/results/butterfly.png\"\n\n    start_time = time.time()\n    test_generator(\n        mask_generator=mask_generator,\n        img_path=img_path,\n        output_path=output_path,\n        rows=1,\n        cols=1,\n        max_mask_crop_region=0.1,\n        show_masks=False\n    )\n    print(f\"Test run took {time.time() - start_time} seconds\")\n\n    gt, pred = read_masks(\"/kaggle/input/evaluation-dataset/masks_set/butterfly.png\", output_path)\n    metrics = evaluate_pred(gt, pred)\n    iou_score = metrics['IoU']\n\n    return iou_score\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=20)\n\nprint(\"Best Hyperparameters:\", study.best_params)\nprint(\"Best IoU Score:\", study.best_value)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}